{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception : Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications, Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Training_Data'\n",
    "validation_data_dir = 'Data/Validation_Data'\n",
    "test_data_dir= 'Data/Test_Data'\n",
    "\n",
    "training_features_file = 'Features/training_features_Inception.npy'\n",
    "validation_features_file = 'Features/validation_features_Inception.npy'\n",
    "top_weights_file = 'Weights/weights_Inception.h5'\n",
    "model_file = 'Models/model_inception.h5'\n",
    "\n",
    "train_labels_file = 'Labels/training_labels.npy'\n",
    "validation_labels_file = 'Labels/validation_labels.npy'\n",
    "test_labels_file = 'Labels/test_labels.npy'\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "NB_CLASSES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(open(train_labels_file, 'rb'))\n",
    "validation_labels = np.load(open(validation_labels_file, 'rb'))\n",
    "test_labels = np.load(open(test_labels_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 307 Images\n",
      "Validation Data : 74 Images\n",
      "Test Data : 18 Images\n"
     ]
    }
   ],
   "source": [
    "print('Training Data : ' + str(len(train_labels)) + ' Images')\n",
    "print('Validation Data : ' + str(len(validation_labels)) + ' Images')\n",
    "print('Test Data : ' + str(len(test_labels)) + ' Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting images to feature vectors using weights from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_feature_vectors(model, directory, batch_size, steps):\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False) # Keep the data in the same order\n",
    "    \n",
    "    features = model.predict_generator(generator, steps, verbose=1) \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Found 307 images belonging to 11 classes.\n",
      "307/307 [==============================] - 27s 88ms/step\n",
      "Found 74 images belonging to 11 classes.\n",
      "74/74 [==============================] - 6s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Batch size has to be a multiple of the number of images  to keep our vectors consistents\n",
    "training_batch_size = 1 # batch size for feature pre-training\n",
    "validation_batch_size = 1 # batch size for feature pre-training\n",
    "\n",
    "model = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))\n",
    "training_features = images_to_feature_vectors(model, train_data_dir, training_batch_size, len(train_labels) // training_batch_size)\n",
    "validation_features = images_to_feature_vectors(model, validation_data_dir, validation_batch_size, len(validation_labels) // validation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_features_file, 'wb') as file:\n",
    "        np.save(file, training_features, allow_pickle = False)\n",
    "with open(validation_features_file, 'wb') as file:\n",
    "        np.save(file, validation_features, allow_pickle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search CV training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr, decay, nn1, nn2, nn3, input_shape, output_shape):\n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=lr, decay=decay)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nn1, input_dim = input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate values\n",
    "lr=[1e-2, 1e-3, 1e-4]\n",
    "decay=[1e-6,1e-9,0]\n",
    "\n",
    "# Number of neurons per layer\n",
    "nn1=[4096,2048,1024]\n",
    "nn2=[2048,1024,512]\n",
    "nn3=[1000,500,200]\n",
    "\n",
    "batch_size=[2048,1024,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open(training_features_file, 'rb'))\n",
    "#train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "validation_data = np.load(open(validation_features_file, 'rb'))\n",
    "#validation_data = validation_data.reshape(validation_data.shape[0],-1)\n",
    "    \n",
    "train_labels_onehot = to_categorical(train_labels,NB_CLASSES)            #One Hot Encoder\n",
    "validation_labels_onehot = to_categorical(validation_labels,NB_CLASSES)  #One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary summary\n",
    "param_grid = dict(\n",
    "                    lr=lr, decay=decay, nn1=nn1, nn2=nn2, nn3=nn3,\n",
    "                    batch_size=batch_size,\n",
    "                    input_shape=train_data.shape[1:], output_shape = (NB_CLASSES,)\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=param_grid, \n",
    "                          verbose=20,  n_iter=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 22s 10ms/step - loss: 5.1505 - acc: 0.0237\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.5874 - acc: 0.2167\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.2660 - acc: 0.2696\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.1233 - acc: 0.3023\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.0248 - acc: 0.3259\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.9086 - acc: 0.3567\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.8035 - acc: 0.3751\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.6899 - acc: 0.4016\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.5944 - acc: 0.4182\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4828 - acc: 0.4428\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4119 - acc: 0.4470\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3392 - acc: 0.4678\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.2431 - acc: 0.4853\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.1658 - acc: 0.5028\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0719 - acc: 0.5345\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0299 - acc: 0.5383\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048, score=0.000, total= 2.7min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 9ms/step - loss: 5.2049 - acc: 0.0147\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.6633 - acc: 0.1887\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.2684 - acc: 0.2668\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.1194 - acc: 0.2881\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.0068 - acc: 0.3155\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.8588 - acc: 0.3562\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.7515 - acc: 0.3713\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.6535 - acc: 0.3955\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.5664 - acc: 0.4092\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4865 - acc: 0.4484\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4172 - acc: 0.4669\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3094 - acc: 0.4868\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.2259 - acc: 0.4967\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.1613 - acc: 0.4981\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0748 - acc: 0.5180\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.9920 - acc: 0.5435\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048, score=0.000, total= 2.7min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 9ms/step - loss: 5.1895 - acc: 0.0161\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.8109 - acc: 0.1656\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.4475 - acc: 0.2436\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.2874 - acc: 0.2706\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 3.1860 - acc: 0.2881\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 3.0891 - acc: 0.3169\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.9774 - acc: 0.3595\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.8791 - acc: 0.3699\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.7687 - acc: 0.3836\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.6700 - acc: 0.3997\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.5735 - acc: 0.4215\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4671 - acc: 0.4423\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3712 - acc: 0.4622\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.2901 - acc: 0.4853\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.2080 - acc: 0.5038\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.1244 - acc: 0.5166\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=4096, lr=0.001, input_shape=5, decay=1e-06, batch_size=2048, score=0.000, total= 2.7min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  8.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 28s 13ms/step - loss: 6.3563 - acc: 0.0501\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 5.9431 - acc: 0.0851\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 5.1081 - acc: 0.0866\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.6787 - acc: 0.1216\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.3208 - acc: 0.1518\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 3.9703 - acc: 0.1755\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.6386 - acc: 0.1939\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.4672 - acc: 0.2171\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 3.2830 - acc: 0.2323\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.0367 - acc: 0.2961\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 2.8440 - acc: 0.3051\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 2.7242 - acc: 0.3245\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.5471 - acc: 0.3699\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.2646 - acc: 0.4191\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.1598 - acc: 0.4494\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 1.9152 - acc: 0.5066\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512, score=0.000, total= 4.9min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 13.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 29s 14ms/step - loss: 6.2776 - acc: 0.0440\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 5.3978 - acc: 0.1003\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 4.8629 - acc: 0.1041\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.4043 - acc: 0.1325\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.0639 - acc: 0.1651\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 3.8474 - acc: 0.1783\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.6490 - acc: 0.2034\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.2725 - acc: 0.2502\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.0197 - acc: 0.2833\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 2.7956 - acc: 0.3226\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.6246 - acc: 0.3439\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.3348 - acc: 0.3926\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 2.1957 - acc: 0.4352\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.0576 - acc: 0.4626\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 1.8978 - acc: 0.4924\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 1.7220 - acc: 0.5317\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512, score=0.001, total= 4.9min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 28s 13ms/step - loss: 6.4410 - acc: 0.0407\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 6.2675 - acc: 0.0572\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 5.2764 - acc: 0.0681\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 4.6231 - acc: 0.1064\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.3845 - acc: 0.1329\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 4.0248 - acc: 0.1495\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.8670 - acc: 0.1556\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 3.6039 - acc: 0.1845\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 3.4066 - acc: 0.2124\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.2343 - acc: 0.2190\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 3.0677 - acc: 0.2498\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.8193 - acc: 0.2947\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 2.7636 - acc: 0.3037\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 2.4979 - acc: 0.3657\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 2.4346 - acc: 0.3628\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 2.2447 - acc: 0.4130\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.01, input_shape=5, decay=1e-06, batch_size=512, score=0.000, total= 5.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 23.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 31s 15ms/step - loss: 4.4930 - acc: 0.1235\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 1.7408 - acc: 0.5889\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 1.0101 - acc: 0.8056\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.6105 - acc: 0.9054\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.3607 - acc: 0.9588\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.2237 - acc: 0.9825\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.1386 - acc: 0.9920\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0983 - acc: 0.9939\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0707 - acc: 0.9953\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0565 - acc: 0.9972\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0337 - acc: 0.9995\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0270 - acc: 0.9986\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0088 - acc: 1.0000\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512, score=0.001, total= 5.2min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 28.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 33s 15ms/step - loss: 4.5017 - acc: 0.1258\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 1.6730 - acc: 0.6050\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.9209 - acc: 0.8264\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.5378 - acc: 0.9290\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.3135 - acc: 0.9659\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.1794 - acc: 0.9896\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.1042 - acc: 0.9976\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0639 - acc: 1.0000\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0423 - acc: 0.9995\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0301 - acc: 1.0000\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0173 - acc: 0.9995\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0087 - acc: 1.0000\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512, score=0.000, total= 5.2min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 33.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 33s 15ms/step - loss: 4.6423 - acc: 0.1050\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 1.8469 - acc: 0.5591\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 1.0915 - acc: 0.7909\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.6562 - acc: 0.9073\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.3872 - acc: 0.9626\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.2318 - acc: 0.9806\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.1441 - acc: 0.9896\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0862 - acc: 0.9981\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0594 - acc: 0.9991\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0387 - acc: 0.9995\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0279 - acc: 1.0000\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0221 - acc: 0.9995\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0102 - acc: 1.0000\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=4096, lr=0.0001, input_shape=2048, decay=1e-09, batch_size=512, score=0.000, total= 5.2min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 38.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 5.0395 - acc: 0.0520\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.7474 - acc: 0.3822\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.1629 - acc: 0.5184\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.7870 - acc: 0.6121\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.4943 - acc: 0.6868\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.2759 - acc: 0.7569\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.0850 - acc: 0.8084\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9198 - acc: 0.8420\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7985 - acc: 0.8657\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.6792 - acc: 0.8917\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5847 - acc: 0.9177\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5048 - acc: 0.9428\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4127 - acc: 0.9612\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3477 - acc: 0.9697\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2863 - acc: 0.9801\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2345 - acc: 0.9872\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024, score=0.003, total= 2.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 40.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 5.0311 - acc: 0.0492\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.6891 - acc: 0.3884\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.1206 - acc: 0.5303\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.7479 - acc: 0.6301\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.4692 - acc: 0.7077\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.2374 - acc: 0.7658\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.0490 - acc: 0.8065\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.8809 - acc: 0.8486\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7580 - acc: 0.8794\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.6374 - acc: 0.9054\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5360 - acc: 0.9333\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4455 - acc: 0.9508\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3671 - acc: 0.9650\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2969 - acc: 0.9759\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2505 - acc: 0.9834\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2061 - acc: 0.9920\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 42.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 5.0151 - acc: 0.0402\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.7890 - acc: 0.3680\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.2364 - acc: 0.5005\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.8717 - acc: 0.6008\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.5827 - acc: 0.6755\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.3531 - acc: 0.7308\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.1578 - acc: 0.7862\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9740 - acc: 0.8307\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.8280 - acc: 0.8718\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.7111 - acc: 0.8964\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5988 - acc: 0.9257\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5196 - acc: 0.9428\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4502 - acc: 0.9546\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3751 - acc: 0.9745\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3154 - acc: 0.9839\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2698 - acc: 0.9872\n",
      "1057/1057 [==============================] - 3s 2ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=2048, lr=0.0001, input_shape=2048, decay=1e-06, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 44.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 10ms/step - loss: 4.6463 - acc: 0.1079\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.8888 - acc: 0.3184\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.3519 - acc: 0.4153\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.8542 - acc: 0.5341\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.5088 - acc: 0.5970\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.1828 - acc: 0.6897\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.9318 - acc: 0.7625\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.7132 - acc: 0.8245\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.5289 - acc: 0.8817\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.4033 - acc: 0.9191\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.3193 - acc: 0.9328\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.2653 - acc: 0.9456\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1635 - acc: 0.9763\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1237 - acc: 0.9844\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0817 - acc: 0.9934\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0584 - acc: 0.9957\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512, score=0.001, total= 2.8min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 47.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 4.5903 - acc: 0.1216\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.9186 - acc: 0.3297\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.2231 - acc: 0.4447\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 1.8072 - acc: 0.5378\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.4462 - acc: 0.6343\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.1387 - acc: 0.7039\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.8820 - acc: 0.7805\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.6549 - acc: 0.8486\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.4650 - acc: 0.9044\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.3228 - acc: 0.9395\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1970 - acc: 0.9697\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1494 - acc: 0.9763\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1177 - acc: 0.9844\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0948 - acc: 0.9877\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0666 - acc: 0.9957\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0497 - acc: 0.9962\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512, score=0.000, total= 2.9min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 50.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 4.6629 - acc: 0.0913\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.9960 - acc: 0.2739\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.3654 - acc: 0.4210\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.9657 - acc: 0.5038\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.5728 - acc: 0.6003\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.2364 - acc: 0.6944\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.9405 - acc: 0.7739\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.6982 - acc: 0.8406\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.5483 - acc: 0.8817\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.4055 - acc: 0.9115\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.2899 - acc: 0.9465\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1974 - acc: 0.9683\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1583 - acc: 0.9773\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.1185 - acc: 0.9858\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0836 - acc: 0.9910\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.0560 - acc: 0.9953\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=1e-09, batch_size=512, score=0.001, total= 2.8min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 53.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 16s 8ms/step - loss: 4.6925 - acc: 0.0762\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.2705 - acc: 0.2677\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.8127 - acc: 0.3841\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.5906 - acc: 0.4276\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.3585 - acc: 0.4768\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.1721 - acc: 0.5180\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.9599 - acc: 0.5658\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.7780 - acc: 0.6055\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.6103 - acc: 0.6462\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.4915 - acc: 0.6727\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.3407 - acc: 0.7062\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.2226 - acc: 0.7417\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.0970 - acc: 0.7758\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.0067 - acc: 0.7985\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.8955 - acc: 0.8127\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7954 - acc: 0.8482\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 55.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 16s 8ms/step - loss: 4.7287 - acc: 0.0695\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.2137 - acc: 0.3009\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.8161 - acc: 0.3784\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.5444 - acc: 0.4352\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.3330 - acc: 0.4678\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.0878 - acc: 0.5251\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.9355 - acc: 0.5700\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.7604 - acc: 0.6026\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.6122 - acc: 0.6443\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.4682 - acc: 0.6887\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.3235 - acc: 0.7110\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.2007 - acc: 0.7469\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.0698 - acc: 0.7838\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9631 - acc: 0.8037\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.8528 - acc: 0.8373\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.7700 - acc: 0.8595\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 57.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.8122 - acc: 0.0530\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 3.3543 - acc: 0.2465\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.9405 - acc: 0.3307\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.6881 - acc: 0.3841\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.4994 - acc: 0.4281\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.2955 - acc: 0.4915\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.1099 - acc: 0.5289\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.9392 - acc: 0.5634\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.7679 - acc: 0.6112\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.6239 - acc: 0.6362\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.5113 - acc: 0.6618\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.3407 - acc: 0.7143\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.2286 - acc: 0.7498\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.0963 - acc: 0.7942\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9663 - acc: 0.8184\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.8753 - acc: 0.8401\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=2048, lr=0.001, input_shape=2048, decay=0, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 59.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 6.4655 - acc: 0.0341\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 6.8951 - acc: 0.0653\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.5641 - acc: 0.0766\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.1779 - acc: 0.0833\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.9766 - acc: 0.0918\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.6943 - acc: 0.1168\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.5522 - acc: 0.1225\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.4067 - acc: 0.1244\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.3057 - acc: 0.1348\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.9376 - acc: 0.1575\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.7812 - acc: 0.1774\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.7581 - acc: 0.1708\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.5817 - acc: 0.1921\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.4998 - acc: 0.2100\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.3720 - acc: 0.2214\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.2810 - acc: 0.2285\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024, score=0.000, total= 2.1min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 61.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 6.1858 - acc: 0.0412\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 7.0385 - acc: 0.0596\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.7707 - acc: 0.0747\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.1177 - acc: 0.1036\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.0689 - acc: 0.0922\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.6284 - acc: 0.1102\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.3481 - acc: 0.1168\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.3381 - acc: 0.1126\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.1661 - acc: 0.1400\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.0987 - acc: 0.1599\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.9743 - acc: 0.1471\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.7501 - acc: 0.1722\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.6076 - acc: 0.1812\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.6123 - acc: 0.1741\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.3975 - acc: 0.2138\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.4364 - acc: 0.2308\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024, score=0.000, total= 2.1min\n",
      "[CV] output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 5.9945 - acc: 0.0307\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 7.1803 - acc: 0.0615\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 6.1399 - acc: 0.0658\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.4052 - acc: 0.0795\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.0547 - acc: 0.0984\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.0506 - acc: 0.0795\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 5.0556 - acc: 0.0998\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.7781 - acc: 0.0927\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.3803 - acc: 0.1107\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.2562 - acc: 0.1457\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 4.1041 - acc: 0.1651\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.9849 - acc: 0.1509\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.8204 - acc: 0.1632\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.5737 - acc: 0.1921\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.6029 - acc: 0.1996\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 3.5378 - acc: 0.1935\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=2048, nn1=2048, lr=0.01, input_shape=2048, decay=1e-09, batch_size=1024, score=0.000, total= 2.1min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 4.6738 - acc: 0.1079\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 2.8477 - acc: 0.3217\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.2039 - acc: 0.4636\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.7289 - acc: 0.5596\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.3323 - acc: 0.6528\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 1.0520 - acc: 0.7219\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.7864 - acc: 0.8070\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.5882 - acc: 0.8642\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.3993 - acc: 0.9219\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2844 - acc: 0.9480\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2028 - acc: 0.9711\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1510 - acc: 0.9816\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.1086 - acc: 0.9872\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0931 - acc: 0.9863\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0710 - acc: 0.9939\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0630 - acc: 0.9920\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512, score=0.000, total= 1.7min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 4.4909 - acc: 0.1230\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 2.7860 - acc: 0.3425\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 2.0854 - acc: 0.4905\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.6803 - acc: 0.5676\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 1.2830 - acc: 0.6618\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.9675 - acc: 0.7621\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.7205 - acc: 0.8330\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.5059 - acc: 0.8874\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.3409 - acc: 0.9342\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2388 - acc: 0.9697\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.1587 - acc: 0.9749\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.1346 - acc: 0.9853\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0993 - acc: 0.9877\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0843 - acc: 0.9886\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0776 - acc: 0.9853\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1029 - acc: 0.9830\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512, score=0.000, total= 1.7min\n",
      "[CV] output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 4.6809 - acc: 0.0889\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 2.9427 - acc: 0.3079\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 2.3233 - acc: 0.4111\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 1.8426 - acc: 0.5289\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 1.4402 - acc: 0.6343\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 1.0990 - acc: 0.7086\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.8184 - acc: 0.7933\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.5883 - acc: 0.8647\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.4489 - acc: 0.9068\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.2941 - acc: 0.9480\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.2043 - acc: 0.9683\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.1537 - acc: 0.9782\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.1123 - acc: 0.9858\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0809 - acc: 0.9920\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0604 - acc: 0.9943\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0397 - acc: 0.9981\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=1024, nn1=1024, lr=0.001, input_shape=5, decay=0, batch_size=512, score=0.000, total= 1.7min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 25s 12ms/step - loss: 5.4051 - acc: 0.0080\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.8341 - acc: 0.3605\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3816 - acc: 0.4811\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.1718 - acc: 0.5322\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.9833 - acc: 0.5752\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.8092 - acc: 0.6192\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.6658 - acc: 0.6523\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.5372 - acc: 0.6764\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4302 - acc: 0.7058\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.3288 - acc: 0.7266\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.2261 - acc: 0.7526\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1262 - acc: 0.7781\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0399 - acc: 0.8004\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.9559 - acc: 0.8250\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8837 - acc: 0.8500\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8203 - acc: 0.8600\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.003, total= 2.9min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 27s 13ms/step - loss: 5.3896 - acc: 0.0137\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.8202 - acc: 0.3784\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3400 - acc: 0.4711\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.1166 - acc: 0.5284\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.9290 - acc: 0.5861\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.7481 - acc: 0.6291\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.5828 - acc: 0.6684\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4476 - acc: 0.7067\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.3439 - acc: 0.7313\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.2432 - acc: 0.7540\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1552 - acc: 0.7725\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0411 - acc: 0.7975\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9377 - acc: 0.8264\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8715 - acc: 0.8420\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8236 - acc: 0.8515\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.7623 - acc: 0.8746\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.001, total= 2.8min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 29s 14ms/step - loss: 5.4391 - acc: 0.0128\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.9759 - acc: 0.3198\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.4886 - acc: 0.4338\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 2.2308 - acc: 0.5099\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0224 - acc: 0.5747\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.8725 - acc: 0.6060\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.7419 - acc: 0.6296\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.6083 - acc: 0.6651\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4729 - acc: 0.6973\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 1.3529 - acc: 0.7252\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.2426 - acc: 0.7474\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1508 - acc: 0.7696\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0521 - acc: 0.8032\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 0.9778 - acc: 0.8184\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9253 - acc: 0.8297\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8576 - acc: 0.8477\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.002, total= 2.9min\n",
      "[CV] output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 14s 6ms/step - loss: 5.3442 - acc: 0.0099\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.6312 - acc: 0.1977\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.1981 - acc: 0.3103\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.9476 - acc: 0.3751\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.7556 - acc: 0.4158\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.5868 - acc: 0.4574\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.4314 - acc: 0.4967\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.2997 - acc: 0.5251\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 3s 2ms/step - loss: 2.1737 - acc: 0.5596\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.0590 - acc: 0.5922\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.9515 - acc: 0.6173\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.8566 - acc: 0.6481\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.7577 - acc: 0.6741\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.6511 - acc: 0.6963\n",
      "Epoch 15/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.5560 - acc: 0.7162\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.4792 - acc: 0.7450\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.004, total= 1.1min\n",
      "[CV] output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 5.2600 - acc: 0.0151\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 3s 2ms/step - loss: 3.5396 - acc: 0.2318\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.0783 - acc: 0.3335\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.8476 - acc: 0.3907\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.6631 - acc: 0.4418\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.4931 - acc: 0.4834\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.3242 - acc: 0.5364\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.1908 - acc: 0.5672\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.0773 - acc: 0.5951\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 3s 2ms/step - loss: 1.9649 - acc: 0.6239\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.8518 - acc: 0.6447\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.7396 - acc: 0.6741\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.6401 - acc: 0.6991\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.5505 - acc: 0.7285\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.4793 - acc: 0.7441\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.3923 - acc: 0.7625\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.008, total= 1.1min\n",
      "[CV] output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 5.2412 - acc: 0.0109\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.6921 - acc: 0.1887\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.3045 - acc: 0.2843\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 3.0936 - acc: 0.3401\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.9558 - acc: 0.3713\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.8027 - acc: 0.4186\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.6414 - acc: 0.4593\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.4896 - acc: 0.5024\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.3624 - acc: 0.5279\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.2437 - acc: 0.5530\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.1376 - acc: 0.5870\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 2.0414 - acc: 0.6093\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.9418 - acc: 0.6372\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.8335 - acc: 0.6599\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.7307 - acc: 0.6916\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 3s 1ms/step - loss: 1.6436 - acc: 0.7072\n",
      "1057/1057 [==============================] - 5s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=2048, nn1=1024, lr=0.0001, input_shape=5, decay=1e-06, batch_size=2048, score=0.003, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 82.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "3171/3171 [==============================] - 16s 5ms/step - loss: 5.0564 - acc: 0.0224\n",
      "Epoch 2/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 3.5319 - acc: 0.2157\n",
      "Epoch 3/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 2.8032 - acc: 0.4056\n",
      "Epoch 4/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 2.2595 - acc: 0.5746\n",
      "Epoch 5/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 1.8148 - acc: 0.6982\n",
      "Epoch 6/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 1.4618 - acc: 0.7997\n",
      "Epoch 7/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 1.1583 - acc: 0.8764\n",
      "Epoch 8/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.9117 - acc: 0.9262\n",
      "Epoch 9/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.7109 - acc: 0.9644\n",
      "Epoch 10/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.5514 - acc: 0.9842\n",
      "Epoch 11/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.4209 - acc: 0.9924\n",
      "Epoch 12/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.3267 - acc: 0.9959\n",
      "Epoch 13/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.2553 - acc: 0.9978\n",
      "Epoch 14/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.2016 - acc: 0.9994\n",
      "Epoch 15/16\n",
      "3171/3171 [==============================] - 4s 1ms/step - loss: 0.1614 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "3171/3171 [==============================] - 3s 1ms/step - loss: 0.1313 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(train_data, train_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "cv_results_df.to_csv('gridsearch_Inception.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_output_shape</th>\n",
       "      <th>param_nn3</th>\n",
       "      <th>param_nn2</th>\n",
       "      <th>param_nn1</th>\n",
       "      <th>param_lr</th>\n",
       "      <th>param_input_shape</th>\n",
       "      <th>param_decay</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160.906361</td>\n",
       "      <td>1.215342</td>\n",
       "      <td>2.035298</td>\n",
       "      <td>0.069349</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293.947517</td>\n",
       "      <td>0.827491</td>\n",
       "      <td>2.931484</td>\n",
       "      <td>0.117531</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 512,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310.711635</td>\n",
       "      <td>0.515874</td>\n",
       "      <td>3.228173</td>\n",
       "      <td>0.139815</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>2048</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2048</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 2048...</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116.765649</td>\n",
       "      <td>0.845978</td>\n",
       "      <td>2.688265</td>\n",
       "      <td>0.348304</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2048</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 512,...</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.962746</td>\n",
       "      <td>1.540708</td>\n",
       "      <td>3.492018</td>\n",
       "      <td>0.524037</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2048</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 1024...</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116.682970</td>\n",
       "      <td>0.543197</td>\n",
       "      <td>3.068732</td>\n",
       "      <td>0.081720</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>123.698080</td>\n",
       "      <td>0.398950</td>\n",
       "      <td>3.486437</td>\n",
       "      <td>0.207097</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2048</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 2048...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>99.318020</td>\n",
       "      <td>0.970573</td>\n",
       "      <td>3.719592</td>\n",
       "      <td>0.198820</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 1024...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>168.245318</td>\n",
       "      <td>1.524959</td>\n",
       "      <td>4.409509</td>\n",
       "      <td>0.084696</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 1024,...</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.198568</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>4.392888</td>\n",
       "      <td>0.214267</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 2048,...</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     160.906361      1.215342         2.035298        0.069349   \n",
       "1     293.947517      0.827491         2.931484        0.117531   \n",
       "2     310.711635      0.515874         3.228173        0.139815   \n",
       "3     116.765649      0.845978         2.688265        0.348304   \n",
       "4     166.962746      1.540708         3.492018        0.524037   \n",
       "5     116.682970      0.543197         3.068732        0.081720   \n",
       "6     123.698080      0.398950         3.486437        0.207097   \n",
       "7      99.318020      0.970573         3.719592        0.198820   \n",
       "8     168.245318      1.524959         4.409509        0.084696   \n",
       "9      62.198568      0.730994         4.392888        0.214267   \n",
       "\n",
       "  param_output_shape param_nn3 param_nn2 param_nn1 param_lr param_input_shape  \\\n",
       "0                101       200      1024      4096    0.001                 5   \n",
       "1                101      1000       512      4096     0.01                 5   \n",
       "2                101      1000      2048      4096   0.0001              2048   \n",
       "3                101      1000       512      2048   0.0001              2048   \n",
       "4                101      1000      1024      2048    0.001              2048   \n",
       "5                101       200      1024      2048    0.001              2048   \n",
       "6                101      1000      2048      2048     0.01              2048   \n",
       "7                101      1000      1024      1024    0.001                 5   \n",
       "8                101       500      1024      4096   0.0001                 5   \n",
       "9                101       200      2048      1024   0.0001                 5   \n",
       "\n",
       "  param_decay param_batch_size  \\\n",
       "0       1e-06             2048   \n",
       "1       1e-06              512   \n",
       "2       1e-09              512   \n",
       "3       1e-06             1024   \n",
       "4       1e-09              512   \n",
       "5           0             1024   \n",
       "6       1e-09             1024   \n",
       "7           0              512   \n",
       "8       1e-06             2048   \n",
       "9       1e-06             2048   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'output_shape': 101, 'nn3': 200, 'nn2': 1024,...           0.000000   \n",
       "1  {'output_shape': 101, 'nn3': 1000, 'nn2': 512,...           0.000000   \n",
       "2  {'output_shape': 101, 'nn3': 1000, 'nn2': 2048...           0.000946   \n",
       "3  {'output_shape': 101, 'nn3': 1000, 'nn2': 512,...           0.002838   \n",
       "4  {'output_shape': 101, 'nn3': 1000, 'nn2': 1024...           0.000946   \n",
       "5  {'output_shape': 101, 'nn3': 200, 'nn2': 1024,...           0.000000   \n",
       "6  {'output_shape': 101, 'nn3': 1000, 'nn2': 2048...           0.000000   \n",
       "7  {'output_shape': 101, 'nn3': 1000, 'nn2': 1024...           0.000000   \n",
       "8  {'output_shape': 101, 'nn3': 500, 'nn2': 1024,...           0.002838   \n",
       "9  {'output_shape': 101, 'nn3': 200, 'nn2': 2048,...           0.003784   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.000000           0.000000         0.000000        0.000000   \n",
       "1           0.000946           0.000000         0.000315        0.000446   \n",
       "2           0.000000           0.000000         0.000315        0.000446   \n",
       "3           0.000000           0.000000         0.000946        0.001338   \n",
       "4           0.000000           0.000946         0.000631        0.000446   \n",
       "5           0.000000           0.000000         0.000000        0.000000   \n",
       "6           0.000000           0.000000         0.000000        0.000000   \n",
       "7           0.000000           0.000000         0.000000        0.000000   \n",
       "8           0.000946           0.001892         0.001892        0.000772   \n",
       "9           0.007569           0.002838         0.004730        0.002044   \n",
       "\n",
       "   rank_test_score  \n",
       "0                7  \n",
       "1                6  \n",
       "2                5  \n",
       "3                3  \n",
       "4                4  \n",
       "5                7  \n",
       "6                7  \n",
       "7                7  \n",
       "8                2  \n",
       "9                1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_shape': 101,\n",
       " 'nn3': 200,\n",
       " 'nn2': 2048,\n",
       " 'nn1': 1024,\n",
       " 'lr': 0.0001,\n",
       " 'input_shape': 5,\n",
       " 'decay': 1e-06,\n",
       " 'batch_size': 2048}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters after RandomizedSearchCV\n",
    "\n",
    "nn1 = 1024; nn2 = 1024; nn3 = 200\n",
    "lr = 0.0001; decay=1e-06\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularzation Parameters\n",
    "\n",
    "dropout = 0.5\n",
    "l1 = 0.0001\n",
    "l2 = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    opt = keras.optimizers.Adam(lr=lr)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nn1, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn2, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn3, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot = to_categorical(train_labels, NB_CLASSES)            #One Hot Encoder\n",
    "validation_labels_onehot = to_categorical(validation_labels, NB_CLASSES)  #One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 307 samples, validate on 74 samples\n",
      "Epoch 1/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 27.8003 - acc: 0.6515 - val_loss: 27.8392 - val_acc: 0.5541\n",
      "Epoch 2/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 27.6151 - acc: 0.6352 - val_loss: 27.6577 - val_acc: 0.5270\n",
      "Epoch 3/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 27.4293 - acc: 0.6352 - val_loss: 27.4794 - val_acc: 0.5270\n",
      "Epoch 4/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 27.2699 - acc: 0.6450 - val_loss: 27.2951 - val_acc: 0.5270\n",
      "Epoch 5/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 27.0522 - acc: 0.6580 - val_loss: 27.1135 - val_acc: 0.5405\n",
      "Epoch 6/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 26.8089 - acc: 0.6612 - val_loss: 26.9316 - val_acc: 0.5270\n",
      "Epoch 7/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 26.6344 - acc: 0.6808 - val_loss: 26.7473 - val_acc: 0.5270\n",
      "Epoch 8/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 26.4207 - acc: 0.7003 - val_loss: 26.5644 - val_acc: 0.5405\n",
      "Epoch 9/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 26.2386 - acc: 0.6971 - val_loss: 26.3908 - val_acc: 0.5541\n",
      "Epoch 10/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 26.0802 - acc: 0.7003 - val_loss: 26.2184 - val_acc: 0.5405\n",
      "Epoch 11/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 25.8896 - acc: 0.7362 - val_loss: 26.0462 - val_acc: 0.5405\n",
      "Epoch 12/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 25.6693 - acc: 0.7264 - val_loss: 25.8731 - val_acc: 0.5270\n",
      "Epoch 13/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 25.5193 - acc: 0.7329 - val_loss: 25.6987 - val_acc: 0.5135\n",
      "Epoch 14/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 25.2729 - acc: 0.7590 - val_loss: 25.5248 - val_acc: 0.5541\n",
      "Epoch 15/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 25.0194 - acc: 0.7883 - val_loss: 25.3528 - val_acc: 0.5676\n",
      "Epoch 16/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 24.8936 - acc: 0.7818 - val_loss: 25.1851 - val_acc: 0.5676\n",
      "Epoch 17/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 24.7468 - acc: 0.7394 - val_loss: 25.0187 - val_acc: 0.5811\n",
      "Epoch 18/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 24.5444 - acc: 0.7915 - val_loss: 24.8544 - val_acc: 0.5541\n",
      "Epoch 19/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 24.4527 - acc: 0.7687 - val_loss: 24.6955 - val_acc: 0.5541\n",
      "Epoch 20/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 24.2434 - acc: 0.7720 - val_loss: 24.5368 - val_acc: 0.5405\n",
      "Epoch 21/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 23.9806 - acc: 0.8274 - val_loss: 24.3842 - val_acc: 0.5405\n",
      "Epoch 22/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 23.9009 - acc: 0.7818 - val_loss: 24.2331 - val_acc: 0.5405\n",
      "Epoch 23/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 23.7047 - acc: 0.7980 - val_loss: 24.0814 - val_acc: 0.5405\n",
      "Epoch 24/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 23.5176 - acc: 0.8274 - val_loss: 23.9310 - val_acc: 0.5405\n",
      "Epoch 25/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 23.3268 - acc: 0.8143 - val_loss: 23.7818 - val_acc: 0.5405\n",
      "Epoch 26/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 23.1771 - acc: 0.8046 - val_loss: 23.6319 - val_acc: 0.5405\n",
      "Epoch 27/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 22.9931 - acc: 0.8241 - val_loss: 23.4823 - val_acc: 0.5270\n",
      "Epoch 28/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 22.8519 - acc: 0.8176 - val_loss: 23.3301 - val_acc: 0.5270\n",
      "Epoch 29/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 22.6686 - acc: 0.8111 - val_loss: 23.1761 - val_acc: 0.5135\n",
      "Epoch 30/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 22.5195 - acc: 0.8339 - val_loss: 23.0180 - val_acc: 0.5000\n",
      "Epoch 31/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 22.3178 - acc: 0.8469 - val_loss: 22.8618 - val_acc: 0.5135\n",
      "Epoch 32/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 22.2055 - acc: 0.8469 - val_loss: 22.7081 - val_acc: 0.5135\n",
      "Epoch 33/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.9737 - acc: 0.8730 - val_loss: 25.5111 - val_acc: 0.3919\n",
      "Epoch 34/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.8068 - acc: 0.8990 - val_loss: 22.4325 - val_acc: 0.5270\n",
      "Epoch 35/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.6497 - acc: 0.8762 - val_loss: 22.2460 - val_acc: 0.5405\n",
      "Epoch 36/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.5162 - acc: 0.8762 - val_loss: 22.0886 - val_acc: 0.5946\n",
      "Epoch 37/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.3146 - acc: 0.9121 - val_loss: 21.9407 - val_acc: 0.5811\n",
      "Epoch 38/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 21.2010 - acc: 0.8795 - val_loss: 21.8001 - val_acc: 0.5811\n",
      "Epoch 39/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 21.0874 - acc: 0.8664 - val_loss: 21.6569 - val_acc: 0.5811\n",
      "Epoch 40/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.8890 - acc: 0.9218 - val_loss: 21.5141 - val_acc: 0.5811\n",
      "Epoch 41/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.7301 - acc: 0.8860 - val_loss: 21.3739 - val_acc: 0.5811\n",
      "Epoch 42/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.6257 - acc: 0.9088 - val_loss: 21.2363 - val_acc: 0.5811\n",
      "Epoch 43/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.4510 - acc: 0.9055 - val_loss: 21.1023 - val_acc: 0.5946\n",
      "Epoch 44/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 20.2902 - acc: 0.9186 - val_loss: 20.9661 - val_acc: 0.5946\n",
      "Epoch 45/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.1518 - acc: 0.9316 - val_loss: 20.8296 - val_acc: 0.5811\n",
      "Epoch 46/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 20.0032 - acc: 0.9218 - val_loss: 20.7000 - val_acc: 0.5811\n",
      "Epoch 47/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 19.8721 - acc: 0.9316 - val_loss: 20.5734 - val_acc: 0.5811\n",
      "Epoch 48/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 19.7440 - acc: 0.9381 - val_loss: 20.4484 - val_acc: 0.5811\n",
      "Epoch 49/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 19.5737 - acc: 0.9381 - val_loss: 20.3269 - val_acc: 0.5405\n",
      "Epoch 50/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 19.4270 - acc: 0.9381 - val_loss: 20.2039 - val_acc: 0.5676\n",
      "Epoch 51/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 19.2997 - acc: 0.9381 - val_loss: 20.0852 - val_acc: 0.5676\n",
      "Epoch 52/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 19.1754 - acc: 0.9479 - val_loss: 25.0151 - val_acc: 0.2432\n",
      "Epoch 53/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 19.0231 - acc: 0.9479 - val_loss: 19.8588 - val_acc: 0.6081\n",
      "Epoch 54/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 18.9014 - acc: 0.9414 - val_loss: 19.7175 - val_acc: 0.5676\n",
      "Epoch 55/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 18.7617 - acc: 0.9511 - val_loss: 19.5958 - val_acc: 0.5676\n",
      "Epoch 56/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 18.6552 - acc: 0.9577 - val_loss: 19.4764 - val_acc: 0.5811\n",
      "Epoch 57/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 18.5154 - acc: 0.9642 - val_loss: 19.3527 - val_acc: 0.5676\n",
      "Epoch 58/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 18.4141 - acc: 0.9609 - val_loss: 19.2284 - val_acc: 0.5676\n",
      "Epoch 59/64\n",
      "307/307 [==============================] - 2s 5ms/step - loss: 18.2828 - acc: 0.9414 - val_loss: 19.1096 - val_acc: 0.5676\n",
      "Epoch 60/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 18.1845 - acc: 0.9186 - val_loss: 18.9927 - val_acc: 0.5676\n",
      "Epoch 61/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 18.0267 - acc: 0.9511 - val_loss: 18.8792 - val_acc: 0.5676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 17.8838 - acc: 0.9674 - val_loss: 18.7671 - val_acc: 0.5811\n",
      "Epoch 63/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 17.7907 - acc: 0.9414 - val_loss: 18.6497 - val_acc: 0.5676\n",
      "Epoch 64/64\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 17.6851 - acc: 0.9577 - val_loss: 18.5336 - val_acc: 0.5676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ce1dc8c88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top.fit(train_data, train_labels_onehot,\n",
    "              epochs=64,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.save_weights(top_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model():\n",
    "    opt = keras.optimizers.Adam(lr=lr)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                 \n",
    "    base = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))\n",
    "    \n",
    "    top = Sequential()\n",
    "\n",
    "    top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    top.add(Dense(nn1, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())\n",
    "    top.add(Dense(nn2, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())\n",
    "    top.add(Dense(nn3, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())            \n",
    "    top.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "    top.load_weights(top_weights_file)\n",
    "    top.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    \n",
    "    \n",
    "    model = Model(input= base.input, output= top(base.output))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 111, 111, 32) 864         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 111, 111, 32) 96          conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 111, 111, 32) 0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 109, 109, 32) 9216        activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 109, 109, 32) 96          conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 109, 109, 32) 0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 109, 109, 64) 18432       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 109, 109, 64) 192         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 109, 109, 64) 0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 54, 54, 80)   240         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 54, 54, 80)   0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 52, 52, 192)  138240      activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 52, 52, 192)  576         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 52, 52, 192)  0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 25, 25, 192)  0           activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 25, 25, 64)   192         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 25, 25, 64)   0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 25, 25, 96)   55296       activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 25, 25, 48)   144         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 25, 25, 96)   288         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 25, 25, 48)   0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 25, 25, 96)   0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 25, 25, 64)   76800       activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 25, 25, 96)   82944       activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 25, 25, 64)   192         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 25, 25, 64)   192         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 25, 25, 96)   288         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 25, 25, 32)   96          conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 25, 25, 64)   0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 25, 25, 64)   0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 25, 25, 96)   0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 25, 25, 32)   0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_194[0][0]             \n",
      "                                                                 activation_196[0][0]             \n",
      "                                                                 activation_199[0][0]             \n",
      "                                                                 activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 25, 25, 64)   192         conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 25, 25, 64)   0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 25, 25, 96)   55296       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 25, 25, 48)   144         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 25, 25, 96)   288         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 25, 25, 48)   0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 25, 25, 96)   0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 25, 25, 64)   76800       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 25, 25, 96)   82944       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 25, 25, 64)   192         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 25, 25, 64)   192         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 25, 25, 96)   288         conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 25, 25, 64)   192         conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 25, 25, 64)   0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 25, 25, 64)   0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 25, 25, 96)   0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 25, 25, 64)   0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_201[0][0]             \n",
      "                                                                 activation_203[0][0]             \n",
      "                                                                 activation_206[0][0]             \n",
      "                                                                 activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 25, 25, 64)   192         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 25, 25, 64)   0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 25, 25, 96)   55296       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 25, 25, 48)   144         conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 25, 25, 96)   288         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 25, 25, 48)   0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 25, 25, 96)   0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 25, 25, 64)   76800       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 25, 25, 96)   82944       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 25, 25, 64)   192         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 25, 25, 64)   192         conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 25, 25, 96)   288         conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 25, 25, 64)   192         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 25, 25, 64)   0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 25, 25, 64)   0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 25, 25, 96)   0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 25, 25, 64)   0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_208[0][0]             \n",
      "                                                                 activation_210[0][0]             \n",
      "                                                                 activation_213[0][0]             \n",
      "                                                                 activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 25, 25, 64)   192         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 25, 25, 64)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 25, 25, 96)   55296       activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 25, 25, 96)   288         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 25, 25, 96)   0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 12, 12, 96)   82944       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 12, 12, 384)  1152        conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 12, 12, 96)   288         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 12, 12, 384)  0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 12, 12, 96)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_215[0][0]             \n",
      "                                                                 activation_218[0][0]             \n",
      "                                                                 max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 12, 12, 128)  384         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 12, 12, 128)  0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 12, 12, 128)  114688      activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 12, 12, 128)  384         conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 12, 12, 128)  0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 12, 12, 128)  114688      activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 12, 12, 128)  384         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 12, 12, 128)  384         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 12, 12, 128)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 12, 12, 128)  0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 12, 12, 128)  114688      activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 12, 12, 128)  114688      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 12, 12, 128)  384         conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 12, 12, 128)  384         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 12, 12, 128)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 12, 12, 128)  0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 12, 12, 192)  172032      activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 12, 12, 192)  172032      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 12, 12, 192)  576         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 12, 12, 192)  576         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 12, 12, 192)  576         conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 12, 12, 192)  576         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 12, 12, 192)  0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 12, 12, 192)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 12, 12, 192)  0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 12, 12, 192)  0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_219[0][0]             \n",
      "                                                                 activation_222[0][0]             \n",
      "                                                                 activation_227[0][0]             \n",
      "                                                                 activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 12, 12, 160)  480         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 12, 12, 160)  0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 12, 12, 160)  179200      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 12, 12, 160)  480         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 12, 12, 160)  0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 12, 12, 160)  179200      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 12, 12, 160)  480         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 12, 12, 160)  480         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 12, 12, 160)  0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 12, 12, 160)  0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 12, 12, 160)  179200      activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 12, 12, 160)  179200      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 12, 12, 160)  480         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 12, 12, 160)  480         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 12, 12, 160)  0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 12, 12, 160)  0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 12, 12, 192)  215040      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 12, 12, 192)  215040      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 12, 12, 192)  576         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 12, 12, 192)  576         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 12, 12, 192)  576         conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 12, 12, 192)  576         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 12, 12, 192)  0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 12, 12, 192)  0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 12, 12, 192)  0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 12, 12, 192)  0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_229[0][0]             \n",
      "                                                                 activation_232[0][0]             \n",
      "                                                                 activation_237[0][0]             \n",
      "                                                                 activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 12, 12, 160)  480         conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 12, 12, 160)  0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 12, 12, 160)  179200      activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 12, 12, 160)  480         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 12, 12, 160)  0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 12, 12, 160)  179200      activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 12, 12, 160)  480         conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 12, 12, 160)  480         conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 12, 12, 160)  0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 12, 12, 160)  0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 12, 12, 160)  179200      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 12, 12, 160)  179200      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 12, 12, 160)  480         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 12, 12, 160)  480         conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 12, 12, 160)  0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 12, 12, 160)  0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 12, 12, 192)  215040      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 12, 12, 192)  215040      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 12, 12, 192)  576         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 12, 12, 192)  576         conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 12, 12, 192)  576         conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 12, 12, 192)  576         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 12, 12, 192)  0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 12, 12, 192)  0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 12, 12, 192)  0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 12, 12, 192)  0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_239[0][0]             \n",
      "                                                                 activation_242[0][0]             \n",
      "                                                                 activation_247[0][0]             \n",
      "                                                                 activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 12, 12, 192)  576         conv2d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 12, 12, 192)  0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 12, 12, 192)  258048      activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 12, 12, 192)  576         conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 12, 12, 192)  0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 12, 12, 192)  258048      activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 12, 12, 192)  576         conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 12, 12, 192)  576         conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 12, 12, 192)  0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 12, 12, 192)  0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 12, 12, 192)  258048      activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 12, 12, 192)  258048      activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 12, 12, 192)  576         conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, 12, 12, 192)  576         conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 12, 12, 192)  0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 12, 12, 192)  0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_25 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 12, 12, 192)  258048      activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 12, 12, 192)  258048      activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 12, 12, 192)  576         conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 12, 12, 192)  576         conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 12, 12, 192)  576         conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 12, 12, 192)  576         conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 12, 12, 192)  0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 12, 12, 192)  0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 12, 12, 192)  0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 12, 12, 192)  0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_249[0][0]             \n",
      "                                                                 activation_252[0][0]             \n",
      "                                                                 activation_257[0][0]             \n",
      "                                                                 activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, 12, 12, 192)  576         conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 12, 12, 192)  0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, 12, 12, 192)  258048      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 12, 12, 192)  576         conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 12, 12, 192)  0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 12, 12, 192)  258048      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, 12, 12, 192)  576         conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 12, 12, 192)  576         conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 12, 12, 192)  0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 12, 12, 192)  0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 5, 5, 320)    552960      activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 5, 5, 192)    331776      activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, 5, 5, 320)    960         conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 5, 5, 192)    576         conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 5, 5, 320)    0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 5, 5, 192)    0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_260[0][0]             \n",
      "                                                                 activation_264[0][0]             \n",
      "                                                                 max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 5, 5, 448)    1344        conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 5, 5, 448)    0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, 5, 5, 384)    1548288     activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 5, 5, 384)    1152        conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 5, 5, 384)    1152        conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 5, 5, 384)    0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 5, 5, 384)    0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, 5, 5, 384)    442368      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, 5, 5, 384)    442368      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, 5, 5, 384)    442368      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, 5, 5, 384)    442368      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_26 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 5, 5, 384)    1152        conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 5, 5, 384)    1152        conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 5, 5, 384)    1152        conv2d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 5, 5, 384)    1152        conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 5, 5, 320)    960         conv2d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 5, 5, 384)    0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 5, 5, 384)    0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 5, 5, 384)    0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 5, 5, 384)    0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 5, 5, 192)    576         conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 5, 5, 320)    0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_267[0][0]             \n",
      "                                                                 activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 5, 5, 768)    0           activation_271[0][0]             \n",
      "                                                                 activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 5, 5, 192)    0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_265[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_5[0][0]              \n",
      "                                                                 activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 5, 5, 448)    1344        conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 5, 5, 448)    0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 5, 5, 384)    1548288     activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 5, 5, 384)    1152        conv2d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 5, 5, 384)    1152        conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 5, 5, 384)    0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 5, 5, 384)    0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, 5, 5, 384)    442368      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, 5, 5, 384)    442368      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 5, 5, 384)    442368      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, 5, 5, 384)    442368      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 5, 5, 384)    1152        conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 5, 5, 384)    1152        conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 5, 5, 384)    1152        conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 5, 5, 384)    1152        conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 5, 5, 320)    960         conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 5, 5, 384)    0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 5, 5, 384)    0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 5, 5, 384)    0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 5, 5, 384)    0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 5, 5, 192)    576         conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 5, 5, 320)    0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_276[0][0]             \n",
      "                                                                 activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 5, 5, 768)    0           activation_280[0][0]             \n",
      "                                                                 activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 5, 5, 192)    0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_274[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 11)           53695627    mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 75,498,411\n",
      "Trainable params: 0\n",
      "Non-trainable params: 75,498,411\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = complete_model()\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"Model Summary\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 images belonging to 11 classes.\n",
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=len(test_labels),\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "test_predictions = model.predict_generator(generator, 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.asarray(list(map(str,np.argmax(test_predictions,axis=1)))).reshape(-1,1)\n",
    "test_labels = np.asarray(test_labels).reshape(-1,1)\n",
    "test_acc, test_acc_op = tf.metrics.accuracy(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = sess.run(test_acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy : 0.5555556\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest accuracy : ' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
