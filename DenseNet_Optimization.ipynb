{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet : Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications, Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Training_Data'\n",
    "validation_data_dir = 'Data/Validation_Data'\n",
    "test_data_dir= 'Data/Test_Data'\n",
    "\n",
    "training_features_file = 'Features/training_features_Densenet.npy'\n",
    "validation_features_file = 'Features/validation_features_Densenet.npy'\n",
    "top_weights_file = 'Weights/weights_Densenet.h5'\n",
    "model_file = 'Models/model_Densenet.h5'\n",
    "\n",
    "train_labels_file = 'Labels/training_labels.npy'\n",
    "validation_labels_file = 'Labels/validation_labels.npy'\n",
    "test_labels_file = 'Labels/test_labels.npy'\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "NB_CLASSES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(open(train_labels_file, 'rb'))\n",
    "validation_labels = np.load(open(validation_labels_file, 'rb'))\n",
    "test_labels = np.load(open(test_labels_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 307 Images\n",
      "Validation Data : 74 Images\n",
      "Test Data : 18 Images\n"
     ]
    }
   ],
   "source": [
    "print('Training Data : ' + str(len(train_labels)) + ' Images')\n",
    "print('Validation Data : ' + str(len(validation_labels)) + ' Images')\n",
    "print('Test Data : ' + str(len(test_labels)) + ' Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting images to feature vectors using weights from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_feature_vectors(model, directory, batch_size, steps):\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False) # Keep the data in the same order\n",
    "    \n",
    "    features = model.predict_generator(generator, steps, verbose=1) \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 307 images belonging to 11 classes.\n",
      "307/307 [==============================] - 46s 149ms/step\n",
      "Found 74 images belonging to 11 classes.\n",
      "74/74 [==============================] - 10s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "# Batch size has to be a multiple of the number of images  to keep our vectors consistents\n",
    "training_batch_size = 1 # batch size for feature pre-training\n",
    "validation_batch_size = 1 # batch size for feature pre-training\n",
    "\n",
    "model = applications.DenseNet121(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))\n",
    "training_features = images_to_feature_vectors(model, train_data_dir, training_batch_size, len(train_labels) // training_batch_size)\n",
    "validation_features = images_to_feature_vectors(model, validation_data_dir, validation_batch_size, len(validation_labels) // validation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_features_file, 'wb') as file:\n",
    "        np.save(file, training_features, allow_pickle = False)\n",
    "with open(validation_features_file, 'wb') as file:\n",
    "        np.save(file, validation_features, allow_pickle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search CV training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr, decay, nn1, nn2, nn3, input_shape, output_shape):\n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=lr, decay=decay)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nn1, input_dim = input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate values\n",
    "lr=[1e-2, 1e-3, 1e-4]\n",
    "decay=[1e-6,1e-9,0]\n",
    "\n",
    "# Number of neurons per layer\n",
    "nn1=[4096,2048,1024]\n",
    "nn2=[2048,1024,512]\n",
    "nn3=[1000,500,200]\n",
    "\n",
    "batch_size=[2048,1024,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open(training_features_file, 'rb'))\n",
    "#train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "validation_data = np.load(open(validation_features_file, 'rb'))\n",
    "#validation_data = validation_data.reshape(validation_data.shape[0],-1)\n",
    "    \n",
    "train_labels_onehot = to_categorical(train_labels,NB_CLASSES)            #One Hot Encoder\n",
    "validation_labels_onehot = to_categorical(validation_labels,NB_CLASSES)  #One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary summary\n",
    "param_grid = dict(\n",
    "                    lr=lr, decay=decay, nn1=nn1, nn2=nn2, nn3=nn3,\n",
    "                    batch_size=batch_size,\n",
    "                    input_shape=train_data.shape[1:], output_shape = (NB_CLASSES,)\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=param_grid, \n",
    "                          verbose=20,  n_iter=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 10ms/step - loss: 5.4102 - acc: 0.0180\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.9312 - acc: 0.3444\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3313 - acc: 0.4551\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0470 - acc: 0.5303\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.8616 - acc: 0.5667\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.6602 - acc: 0.6244\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4853 - acc: 0.6679\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.3640 - acc: 0.6958\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.2373 - acc: 0.7190\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1283 - acc: 0.7384\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0396 - acc: 0.7507\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9335 - acc: 0.7800\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8692 - acc: 0.7909\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8054 - acc: 0.8061\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.7211 - acc: 0.8344\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.6547 - acc: 0.8557\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048, score=0.001, total= 2.7min\n",
      "[CV] output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 9ms/step - loss: 5.3678 - acc: 0.0189\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.5143 - acc: 0.4513\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.9695 - acc: 0.5643\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.7356 - acc: 0.6154\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.5869 - acc: 0.6481\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4457 - acc: 0.6798\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.3186 - acc: 0.7039\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1868 - acc: 0.7266\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0620 - acc: 0.7621\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9516 - acc: 0.7867\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.8747 - acc: 0.7985\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.7817 - acc: 0.8155\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.6982 - acc: 0.8529\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.6247 - acc: 0.8619\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.5618 - acc: 0.8808\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.5555 - acc: 0.8808\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048, score=0.003, total= 2.6min\n",
      "[CV] output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 20s 9ms/step - loss: 5.3690 - acc: 0.0142\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.7950 - acc: 0.3690\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.3505 - acc: 0.4494\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 2.0664 - acc: 0.5270\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.8652 - acc: 0.5818\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.6589 - acc: 0.6367\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.5315 - acc: 0.6533\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.4056 - acc: 0.6760\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.2713 - acc: 0.7091\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.1637 - acc: 0.7346\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 1.0852 - acc: 0.7498\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9914 - acc: 0.7729\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.9035 - acc: 0.7919\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.7945 - acc: 0.8269\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.7155 - acc: 0.8401\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 9s 4ms/step - loss: 0.6700 - acc: 0.8500\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=512, nn1=4096, lr=0.001, input_shape=7, decay=1e-06, batch_size=2048, score=0.003, total= 2.6min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 4.1839 - acc: 0.1528\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 2.1682 - acc: 0.5629\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 1.5044 - acc: 0.7062\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 1.0096 - acc: 0.8302\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.6868 - acc: 0.9044\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.4478 - acc: 0.9551\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.2910 - acc: 0.9834\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.1850 - acc: 0.9915\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.1167 - acc: 0.9981\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0847 - acc: 0.9967\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0601 - acc: 0.9991\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0301 - acc: 1.0000\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0154 - acc: 1.0000\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512, score=0.000, total= 1.5min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  9.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 10s 5ms/step - loss: 4.0714 - acc: 0.1925\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 2.0315 - acc: 0.5757\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 1.3742 - acc: 0.7465\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.9420 - acc: 0.8486\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.6664 - acc: 0.9035\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.4404 - acc: 0.9518\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.2901 - acc: 0.9763\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.1890 - acc: 0.9905\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.1257 - acc: 0.9953\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0885 - acc: 0.9972\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0625 - acc: 0.9967\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0452 - acc: 0.9991\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0339 - acc: 0.9995\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0169 - acc: 1.0000\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512, score=0.001, total= 1.5min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 4.3142 - acc: 0.1334\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 2.3306 - acc: 0.5194\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 1.6095 - acc: 0.7020\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 1.1527 - acc: 0.8009\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.7672 - acc: 0.9035\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.5169 - acc: 0.9499\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.3351 - acc: 0.9759\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.2099 - acc: 0.9891\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.1417 - acc: 0.9939\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0921 - acc: 0.9991\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0640 - acc: 0.9995\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0484 - acc: 0.9991\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0373 - acc: 1.0000\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 5s 2ms/step - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 5s 3ms/step - loss: 0.0196 - acc: 1.0000\n",
      "1057/1057 [==============================] - 2s 2ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.001, input_shape=7, decay=1e-09, batch_size=512, score=0.002, total= 1.6min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 12.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 30s 14ms/step - loss: 4.0681 - acc: 0.1887\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 1.9963 - acc: 0.5369\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 1.3230 - acc: 0.6906\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.8686 - acc: 0.8051\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.5611 - acc: 0.8926\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.3587 - acc: 0.9428\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.2390 - acc: 0.9640\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.1500 - acc: 0.9806\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0932 - acc: 0.9896\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0617 - acc: 0.9943\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0458 - acc: 0.9957\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0344 - acc: 0.9976\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0331 - acc: 0.9972\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0196 - acc: 0.9991\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0164 - acc: 0.9981\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0125 - acc: 0.9986\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512, score=0.000, total= 5.0min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 17.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 30s 14ms/step - loss: 3.9131 - acc: 0.2256\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 1.7177 - acc: 0.6069\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 1.0889 - acc: 0.7692\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.6827 - acc: 0.8581\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.4150 - acc: 0.9300\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.2779 - acc: 0.9551\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.1631 - acc: 0.9745\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.1252 - acc: 0.9811\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0836 - acc: 0.9886\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0599 - acc: 0.9910\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 19s 9ms/step - loss: 0.0444 - acc: 0.9953\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.0248 - acc: 0.9981\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 0.0175 - acc: 0.9991\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0197 - acc: 0.9981\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0201 - acc: 0.9967\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0136 - acc: 0.9976\n",
      "1057/1057 [==============================] - 4s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512, score=0.001, total= 5.0min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 22.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 30s 14ms/step - loss: 4.1777 - acc: 0.1660\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 1.9585 - acc: 0.5497\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 1.2993 - acc: 0.7214\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.8499 - acc: 0.8160\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.5762 - acc: 0.8898\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.3600 - acc: 0.9447\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.2087 - acc: 0.9763\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.1383 - acc: 0.9816\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0853 - acc: 0.9924\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0630 - acc: 0.9943\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0545 - acc: 0.9957\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0372 - acc: 0.9967\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0356 - acc: 0.9957\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0282 - acc: 0.9967\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 18s 8ms/step - loss: 0.0225 - acc: 0.9967\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 18s 9ms/step - loss: 0.0415 - acc: 0.9957\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=4096, lr=0.001, input_shape=1024, decay=1e-09, batch_size=512, score=0.001, total= 5.1min\n",
      "[CV] output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 27.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 16s 7ms/step - loss: 4.6122 - acc: 0.1325\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.8380 - acc: 0.3936\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.0148 - acc: 0.5454\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.5179 - acc: 0.6500\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.2579 - acc: 0.6968\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.0294 - acc: 0.7502\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.8321 - acc: 0.8027\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.6387 - acc: 0.8562\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5396 - acc: 0.8841\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4239 - acc: 0.9134\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3253 - acc: 0.9366\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2723 - acc: 0.9470\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2217 - acc: 0.9588\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1798 - acc: 0.9711\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1605 - acc: 0.9735\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1241 - acc: 0.9839\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.000, total= 2.0min\n",
      "[CV] output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 29.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 16s 8ms/step - loss: 4.4677 - acc: 0.1377\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.2763 - acc: 0.4868\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.5343 - acc: 0.6547\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.1296 - acc: 0.7422\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.8853 - acc: 0.7900\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7123 - acc: 0.8330\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5915 - acc: 0.8680\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4663 - acc: 0.9063\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4041 - acc: 0.9153\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3253 - acc: 0.9399\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2425 - acc: 0.9560\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1795 - acc: 0.9735\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1392 - acc: 0.9811\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0962 - acc: 0.9891\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0710 - acc: 0.9934\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0657 - acc: 0.9939\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.001, total= 2.1min\n",
      "[CV] output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 31.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 16s 8ms/step - loss: 4.6879 - acc: 0.1003\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.5565 - acc: 0.4040\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.8489 - acc: 0.5620\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.4518 - acc: 0.6556\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.1881 - acc: 0.7219\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9522 - acc: 0.7838\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7929 - acc: 0.8070\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.6487 - acc: 0.8614\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5266 - acc: 0.8907\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4226 - acc: 0.9078\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3382 - acc: 0.9300\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2872 - acc: 0.9442\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2381 - acc: 0.9584\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2042 - acc: 0.9617\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1588 - acc: 0.9797\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1307 - acc: 0.9797\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=2048, nn1=2048, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.002, total= 2.0min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 33.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 4.9178 - acc: 0.0416\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.8368 - acc: 0.4186\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.2045 - acc: 0.6012\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.8101 - acc: 0.6977\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.4995 - acc: 0.7871\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.2569 - acc: 0.8519\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0632 - acc: 0.8898\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.9022 - acc: 0.9248\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.7634 - acc: 0.9451\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6424 - acc: 0.9645\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5429 - acc: 0.9801\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4551 - acc: 0.9891\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3846 - acc: 0.9910\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3287 - acc: 0.9953\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2822 - acc: 0.9981\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2406 - acc: 0.9986\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024, score=0.003, total= 1.2min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 35.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 4.9500 - acc: 0.0435\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.7918 - acc: 0.4328\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.1792 - acc: 0.5932\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.7825 - acc: 0.7029\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.4626 - acc: 0.7848\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.2051 - acc: 0.8467\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0063 - acc: 0.8917\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8371 - acc: 0.9224\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 3s 2ms/step - loss: 0.6983 - acc: 0.9508\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5899 - acc: 0.9631\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5012 - acc: 0.9768\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4310 - acc: 0.9905\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3654 - acc: 0.9929\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3116 - acc: 0.9939\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2669 - acc: 0.9948\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2297 - acc: 0.9967\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024, score=0.000, total= 1.2min\n",
      "[CV] output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 36.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 4.9120 - acc: 0.0317\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.9033 - acc: 0.3846\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.2613 - acc: 0.5922\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.8463 - acc: 0.7237\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.5418 - acc: 0.8042\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.2885 - acc: 0.8657\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0777 - acc: 0.9068\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8949 - acc: 0.9399\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.7478 - acc: 0.9598\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6322 - acc: 0.9759\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5250 - acc: 0.9844\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4397 - acc: 0.9886\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3732 - acc: 0.9939\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3209 - acc: 0.9953\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2762 - acc: 0.9986\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2398 - acc: 0.9991\n",
      "1057/1057 [==============================] - 4s 3ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=512, nn1=1024, lr=0.0001, input_shape=7, decay=1e-09, batch_size=1024, score=0.002, total= 1.2min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 37.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.8448 - acc: 0.0601\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.9640 - acc: 0.5908\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.2813 - acc: 0.7904\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.9208 - acc: 0.8756\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.6695 - acc: 0.9295\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.4917 - acc: 0.9555\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.3581 - acc: 0.9773\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2713 - acc: 0.9858\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2063 - acc: 0.9910\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1596 - acc: 0.9962\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1292 - acc: 0.9976\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1033 - acc: 0.9991\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0843 - acc: 0.9991\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0671 - acc: 0.9995\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0556 - acc: 0.9991\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0478 - acc: 0.9995\n",
      "1057/1057 [==============================] - 3s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024, score=0.001, total= 2.0min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 39.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.8357 - acc: 0.0591\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.8251 - acc: 0.6391\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.1418 - acc: 0.8188\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.7807 - acc: 0.8903\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.5473 - acc: 0.9480\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.3984 - acc: 0.9688\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2972 - acc: 0.9820\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2227 - acc: 0.9924\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1674 - acc: 0.9976\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1259 - acc: 0.9986\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0987 - acc: 0.9995\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0811 - acc: 0.9995\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0659 - acc: 0.9995\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0548 - acc: 0.9991\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0443 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0386 - acc: 1.0000\n",
      "1057/1057 [==============================] - 4s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024, score=0.004, total= 2.0min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 41.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 17s 8ms/step - loss: 4.9825 - acc: 0.0426\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.0437 - acc: 0.5904\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.3398 - acc: 0.7744\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.9226 - acc: 0.8860\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.6594 - acc: 0.9342\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.4625 - acc: 0.9688\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.3273 - acc: 0.9844\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2394 - acc: 0.9915\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1799 - acc: 0.9962\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1415 - acc: 0.9981\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1097 - acc: 0.9986\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0889 - acc: 1.0000\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0754 - acc: 0.9995\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0631 - acc: 0.9995\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0526 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0432 - acc: 1.0000\n",
      "1057/1057 [==============================] - 4s 3ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.0001, input_shape=1024, decay=1e-09, batch_size=1024, score=0.002, total= 2.0min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 43.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 27s 13ms/step - loss: 4.7071 - acc: 0.1192\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 2.5058 - acc: 0.4484\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 1.7985 - acc: 0.5847\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 1.3849 - acc: 0.6731\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 1.1361 - acc: 0.7228\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 13s 6ms/step - loss: 0.8619 - acc: 0.7952\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.6920 - acc: 0.8411\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.5754 - acc: 0.8685\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.4229 - acc: 0.9163\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.3742 - acc: 0.9196\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.3060 - acc: 0.9399\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.2167 - acc: 0.9678\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1858 - acc: 0.9664\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1724 - acc: 0.9693\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1300 - acc: 0.9792\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1308 - acc: 0.9778\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024, score=0.002, total= 3.5min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 46.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 27s 13ms/step - loss: 4.3527 - acc: 0.1599\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 2.0333 - acc: 0.5307\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 1.4647 - acc: 0.6547\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 1.1457 - acc: 0.7271\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.9471 - acc: 0.7777\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.7353 - acc: 0.8278\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.5695 - acc: 0.8756\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.4728 - acc: 0.9016\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.3925 - acc: 0.9229\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.3161 - acc: 0.9376\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.2588 - acc: 0.9503\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.2139 - acc: 0.9588\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1624 - acc: 0.9740\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1571 - acc: 0.9726\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.1099 - acc: 0.9830\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.0833 - acc: 0.9929\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024, score=0.000, total= 3.5min\n",
      "[CV] output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 27s 13ms/step - loss: 4.6519 - acc: 0.1192\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 2.4310 - acc: 0.4357\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 1.7617 - acc: 0.5922\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 1.3777 - acc: 0.6750\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 1.0854 - acc: 0.7526\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 0.8745 - acc: 0.7961\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.7098 - acc: 0.8396\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.5436 - acc: 0.8808\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 0.4229 - acc: 0.9139\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 0.3712 - acc: 0.9210\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.2645 - acc: 0.9494\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 12s 5ms/step - loss: 0.2119 - acc: 0.9626\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 0.1754 - acc: 0.9721\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 11s 5ms/step - loss: 0.1421 - acc: 0.9792\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1099 - acc: 0.9853\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 12s 6ms/step - loss: 0.1088 - acc: 0.9801\n",
      "1057/1057 [==============================] - 5s 4ms/step\n",
      "[CV]  output_shape=101, nn3=1000, nn2=512, nn1=4096, lr=0.001, input_shape=1024, decay=0, batch_size=1024, score=0.003, total= 3.5min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 14s 7ms/step - loss: 4.5500 - acc: 0.1060\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.5285 - acc: 0.4518\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.9871 - acc: 0.5908\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.6032 - acc: 0.6750\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.2972 - acc: 0.7479\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0575 - acc: 0.8051\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8383 - acc: 0.8567\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6841 - acc: 0.8879\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5509 - acc: 0.9219\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4507 - acc: 0.9447\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3565 - acc: 0.9584\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2896 - acc: 0.9707\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2327 - acc: 0.9811\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1899 - acc: 0.9858\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1613 - acc: 0.9882\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1365 - acc: 0.9915\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.000, total= 1.3min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 14s 7ms/step - loss: 4.4499 - acc: 0.1055\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.3018 - acc: 0.5128\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.7368 - acc: 0.6500\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.3922 - acc: 0.7280\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.1236 - acc: 0.7843\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.9333 - acc: 0.8217\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.7526 - acc: 0.8690\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6207 - acc: 0.9030\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4860 - acc: 0.9295\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3844 - acc: 0.9499\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3039 - acc: 0.9693\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2802 - acc: 0.9622\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2162 - acc: 0.9787\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1675 - acc: 0.9868\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1340 - acc: 0.9929\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1079 - acc: 0.9962\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.000, total= 1.3min\n",
      "[CV] output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 14s 7ms/step - loss: 4.6832 - acc: 0.0747\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.6480 - acc: 0.4224\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.0675 - acc: 0.5766\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.6772 - acc: 0.6712\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.3948 - acc: 0.7318\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.1436 - acc: 0.7895\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.9293 - acc: 0.8406\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.7654 - acc: 0.8761\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6171 - acc: 0.9115\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5164 - acc: 0.9352\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3997 - acc: 0.9612\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3299 - acc: 0.9716\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2615 - acc: 0.9816\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2046 - acc: 0.9901\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1666 - acc: 0.9920\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1288 - acc: 0.9976\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=200, nn2=1024, nn1=1024, lr=0.001, input_shape=1024, decay=1e-06, batch_size=1024, score=0.003, total= 1.3min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 14s 7ms/step - loss: 4.6241 - acc: 0.1126\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.2971 - acc: 0.4872\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.6926 - acc: 0.6031\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.3122 - acc: 0.7086\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0283 - acc: 0.7734\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8006 - acc: 0.8278\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6374 - acc: 0.8680\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5163 - acc: 0.8964\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.4098 - acc: 0.9238\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3277 - acc: 0.9470\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2576 - acc: 0.9598\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2014 - acc: 0.9697\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1562 - acc: 0.9797\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1322 - acc: 0.9811\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0959 - acc: 0.9910\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0822 - acc: 0.9915\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024, score=0.000, total= 1.3min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 4.4833 - acc: 0.1367\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.0267 - acc: 0.5397\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.3772 - acc: 0.6968\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.0627 - acc: 0.7621\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8294 - acc: 0.8122\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6412 - acc: 0.8628\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5066 - acc: 0.8974\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4293 - acc: 0.9068\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3408 - acc: 0.9395\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2612 - acc: 0.9560\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2213 - acc: 0.9640\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1814 - acc: 0.9726\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1337 - acc: 0.9806\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1069 - acc: 0.9891\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0910 - acc: 0.9905\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0793 - acc: 0.9891\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024, score=0.000, total= 1.3min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 15s 7ms/step - loss: 4.6349 - acc: 0.0937\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 2.3883 - acc: 0.4404\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.7336 - acc: 0.6126\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.3761 - acc: 0.6831\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 1.1243 - acc: 0.7360\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.8854 - acc: 0.8089\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.6917 - acc: 0.8605\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.5464 - acc: 0.8912\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.4104 - acc: 0.9276\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.3189 - acc: 0.9465\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.2459 - acc: 0.9655\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1774 - acc: 0.9816\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1559 - acc: 0.9820\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.1208 - acc: 0.9882\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0891 - acc: 0.9929\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 4s 2ms/step - loss: 0.0795 - acc: 0.9929\n",
      "1057/1057 [==============================] - 4s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=1024, lr=0.001, input_shape=7, decay=0, batch_size=1024, score=0.003, total= 1.3min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 21s 10ms/step - loss: 4.5625 - acc: 0.1173\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.2472 - acc: 0.4825\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.6505 - acc: 0.6206\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.3165 - acc: 0.6897\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.0477 - acc: 0.7715\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.8559 - acc: 0.7990\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.6556 - acc: 0.8647\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.5610 - acc: 0.8888\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.4450 - acc: 0.9163\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.3597 - acc: 0.9357\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2911 - acc: 0.9541\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2277 - acc: 0.9588\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1790 - acc: 0.9745\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1524 - acc: 0.9749\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1190 - acc: 0.9844\n",
      "Epoch 16/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0817 - acc: 0.9939\n",
      "1057/1057 [==============================] - 5s 4ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024, score=0.000, total= 2.1min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 21s 10ms/step - loss: 4.5174 - acc: 0.1310\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 2.1739 - acc: 0.5095\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.5155 - acc: 0.6429\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 1.1743 - acc: 0.7375\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.9387 - acc: 0.7886\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.7247 - acc: 0.8510\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.5808 - acc: 0.8746\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.4489 - acc: 0.9082\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.3545 - acc: 0.9376\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2916 - acc: 0.9447\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.2242 - acc: 0.9603\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1880 - acc: 0.9763\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1538 - acc: 0.9820\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.1241 - acc: 0.9844\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0935 - acc: 0.9920\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 7s 3ms/step - loss: 0.0893 - acc: 0.9868\n",
      "1057/1057 [==============================] - 5s 5ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024, score=0.002, total= 2.1min\n",
      "[CV] output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024 \n",
      "Epoch 1/16\n",
      "2114/2114 [==============================] - 21s 10ms/step - loss: 4.6569 - acc: 0.1083\n",
      "Epoch 2/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 2.4373 - acc: 0.4503\n",
      "Epoch 3/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.7484 - acc: 0.5842\n",
      "Epoch 4/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.3446 - acc: 0.6930\n",
      "Epoch 5/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 1.0598 - acc: 0.7725\n",
      "Epoch 6/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.8397 - acc: 0.8226\n",
      "Epoch 7/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.6498 - acc: 0.8623\n",
      "Epoch 8/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.5090 - acc: 0.8969\n",
      "Epoch 9/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.4075 - acc: 0.9328\n",
      "Epoch 10/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.3106 - acc: 0.9541\n",
      "Epoch 11/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2502 - acc: 0.9622\n",
      "Epoch 12/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.2281 - acc: 0.9612\n",
      "Epoch 13/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1755 - acc: 0.9730\n",
      "Epoch 14/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1438 - acc: 0.9839\n",
      "Epoch 15/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.1298 - acc: 0.9834\n",
      "Epoch 16/16\n",
      "2114/2114 [==============================] - 6s 3ms/step - loss: 0.0849 - acc: 0.9924\n",
      "1057/1057 [==============================] - 5s 5ms/step\n",
      "[CV]  output_shape=101, nn3=500, nn2=1024, nn1=2048, lr=0.001, input_shape=7, decay=1e-06, batch_size=1024, score=0.002, total= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 67.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "3171/3171 [==============================] - 24s 7ms/step - loss: 4.7408 - acc: 0.0703\n",
      "Epoch 2/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 2.0763 - acc: 0.5762\n",
      "Epoch 3/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 1.3086 - acc: 0.7871\n",
      "Epoch 4/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.8422 - acc: 0.8953\n",
      "Epoch 5/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.5437 - acc: 0.9533\n",
      "Epoch 6/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.3475 - acc: 0.9814\n",
      "Epoch 7/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.2327 - acc: 0.9940\n",
      "Epoch 8/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.1521 - acc: 0.9981\n",
      "Epoch 9/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.1036 - acc: 0.9994\n",
      "Epoch 10/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0740 - acc: 1.0000\n",
      "Epoch 11/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 12/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0449 - acc: 0.9994\n",
      "Epoch 13/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0347 - acc: 0.9997\n",
      "Epoch 14/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 15/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 16/16\n",
      "3171/3171 [==============================] - 9s 3ms/step - loss: 0.0198 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(train_data, train_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "cv_results_df.to_csv('gridsearch_Inception.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_output_shape</th>\n",
       "      <th>param_nn3</th>\n",
       "      <th>param_nn2</th>\n",
       "      <th>param_nn1</th>\n",
       "      <th>param_lr</th>\n",
       "      <th>param_input_shape</th>\n",
       "      <th>param_decay</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156.063100</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>2.285238</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 512, ...</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.610148</td>\n",
       "      <td>1.547271</td>\n",
       "      <td>2.102973</td>\n",
       "      <td>0.107218</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 512, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298.679060</td>\n",
       "      <td>2.168388</td>\n",
       "      <td>3.665428</td>\n",
       "      <td>0.196830</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>512</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119.893126</td>\n",
       "      <td>0.285009</td>\n",
       "      <td>2.857568</td>\n",
       "      <td>0.124893</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 2048,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67.929503</td>\n",
       "      <td>0.982511</td>\n",
       "      <td>3.050566</td>\n",
       "      <td>0.333164</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 512, ...</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115.734195</td>\n",
       "      <td>0.430777</td>\n",
       "      <td>3.421132</td>\n",
       "      <td>0.131229</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1024</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 1024,...</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>205.122791</td>\n",
       "      <td>1.588669</td>\n",
       "      <td>4.554182</td>\n",
       "      <td>0.141909</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 1000, 'nn2': 512,...</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72.048003</td>\n",
       "      <td>0.677030</td>\n",
       "      <td>4.078477</td>\n",
       "      <td>0.144334</td>\n",
       "      <td>101</td>\n",
       "      <td>200</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 200, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73.390035</td>\n",
       "      <td>1.318049</td>\n",
       "      <td>4.293865</td>\n",
       "      <td>0.150274</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>120.233964</td>\n",
       "      <td>1.742860</td>\n",
       "      <td>4.846889</td>\n",
       "      <td>0.212204</td>\n",
       "      <td>101</td>\n",
       "      <td>500</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1024</td>\n",
       "      <td>{'output_shape': 101, 'nn3': 500, 'nn2': 1024,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     156.063100      0.842942         2.285238        0.037467   \n",
       "1      89.610148      1.547271         2.102973        0.107218   \n",
       "2     298.679060      2.168388         3.665428        0.196830   \n",
       "3     119.893126      0.285009         2.857568        0.124893   \n",
       "4      67.929503      0.982511         3.050566        0.333164   \n",
       "5     115.734195      0.430777         3.421132        0.131229   \n",
       "6     205.122791      1.588669         4.554182        0.141909   \n",
       "7      72.048003      0.677030         4.078477        0.144334   \n",
       "8      73.390035      1.318049         4.293865        0.150274   \n",
       "9     120.233964      1.742860         4.846889        0.212204   \n",
       "\n",
       "  param_output_shape param_nn3 param_nn2 param_nn1 param_lr param_input_shape  \\\n",
       "0                101       500       512      4096    0.001                 7   \n",
       "1                101       200       512      1024    0.001                 7   \n",
       "2                101       500      1024      4096    0.001              1024   \n",
       "3                101       500      2048      2048    0.001              1024   \n",
       "4                101       200       512      1024   0.0001                 7   \n",
       "5                101       500      1024      2048   0.0001              1024   \n",
       "6                101      1000       512      4096    0.001              1024   \n",
       "7                101       200      1024      1024    0.001              1024   \n",
       "8                101       500      1024      1024    0.001                 7   \n",
       "9                101       500      1024      2048    0.001                 7   \n",
       "\n",
       "  param_decay param_batch_size  \\\n",
       "0       1e-06             2048   \n",
       "1       1e-09              512   \n",
       "2       1e-09              512   \n",
       "3       1e-06             1024   \n",
       "4       1e-09             1024   \n",
       "5       1e-09             1024   \n",
       "6           0             1024   \n",
       "7       1e-06             1024   \n",
       "8           0             1024   \n",
       "9       1e-06             1024   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'output_shape': 101, 'nn3': 500, 'nn2': 512, ...           0.000946   \n",
       "1  {'output_shape': 101, 'nn3': 200, 'nn2': 512, ...           0.000000   \n",
       "2  {'output_shape': 101, 'nn3': 500, 'nn2': 1024,...           0.000000   \n",
       "3  {'output_shape': 101, 'nn3': 500, 'nn2': 2048,...           0.000000   \n",
       "4  {'output_shape': 101, 'nn3': 200, 'nn2': 512, ...           0.002838   \n",
       "5  {'output_shape': 101, 'nn3': 500, 'nn2': 1024,...           0.000946   \n",
       "6  {'output_shape': 101, 'nn3': 1000, 'nn2': 512,...           0.001892   \n",
       "7  {'output_shape': 101, 'nn3': 200, 'nn2': 1024,...           0.000000   \n",
       "8  {'output_shape': 101, 'nn3': 500, 'nn2': 1024,...           0.000000   \n",
       "9  {'output_shape': 101, 'nn3': 500, 'nn2': 1024,...           0.000000   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.002838           0.002838         0.002208        0.000892   \n",
       "1           0.000946           0.001892         0.000946        0.000772   \n",
       "2           0.000946           0.000946         0.000631        0.000446   \n",
       "3           0.000946           0.001892         0.000946        0.000772   \n",
       "4           0.000000           0.001892         0.001577        0.001180   \n",
       "5           0.003784           0.001892         0.002208        0.001180   \n",
       "6           0.000000           0.002838         0.001577        0.001180   \n",
       "7           0.000000           0.002838         0.000946        0.001338   \n",
       "8           0.000000           0.002838         0.000946        0.001338   \n",
       "9           0.001892           0.001892         0.001261        0.000892   \n",
       "\n",
       "   rank_test_score  \n",
       "0                2  \n",
       "1                6  \n",
       "2               10  \n",
       "3                6  \n",
       "4                4  \n",
       "5                1  \n",
       "6                3  \n",
       "7                8  \n",
       "8                8  \n",
       "9                5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_shape': 101,\n",
       " 'nn3': 500,\n",
       " 'nn2': 1024,\n",
       " 'nn1': 2048,\n",
       " 'lr': 0.0001,\n",
       " 'input_shape': 1024,\n",
       " 'decay': 1e-09,\n",
       " 'batch_size': 1024}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters after RandomizedSearchCV\n",
    "\n",
    "nn1 = 2048; nn2 = 1024; nn3 = 500\n",
    "lr = 0.0001; decay=1e-09\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularzation Parameters\n",
    "\n",
    "dropout = 0.5\n",
    "l1 = 0.0001\n",
    "l2 = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    opt = keras.optimizers.Adam(lr=lr)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nn1, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn2, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nn3, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot = to_categorical(train_labels, NB_CLASSES)            #One Hot Encoder\n",
    "validation_labels_onehot = to_categorical(validation_labels, NB_CLASSES)  #One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 307 samples, validate on 74 samples\n",
      "Epoch 1/64\n",
      "307/307 [==============================] - 32s 103ms/step - loss: 65.1928 - acc: 0.0912 - val_loss: 63.6078 - val_acc: 0.2297\n",
      "Epoch 2/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 64.1822 - acc: 0.1368 - val_loss: 62.6320 - val_acc: 0.4189\n",
      "Epoch 3/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 63.3278 - acc: 0.2117 - val_loss: 61.9100 - val_acc: 0.4730\n",
      "Epoch 4/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 62.4293 - acc: 0.2443 - val_loss: 61.2190 - val_acc: 0.5405\n",
      "Epoch 5/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 61.6148 - acc: 0.3681 - val_loss: 60.5512 - val_acc: 0.6486\n",
      "Epoch 6/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 60.8076 - acc: 0.4365 - val_loss: 59.8947 - val_acc: 0.6892\n",
      "Epoch 7/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 60.3527 - acc: 0.4593 - val_loss: 59.2835 - val_acc: 0.7027\n",
      "Epoch 8/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 59.6567 - acc: 0.4821 - val_loss: 58.6872 - val_acc: 0.7027\n",
      "Epoch 9/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 58.8778 - acc: 0.5570 - val_loss: 58.1017 - val_acc: 0.7162\n",
      "Epoch 10/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 58.2999 - acc: 0.5928 - val_loss: 57.5258 - val_acc: 0.7162\n",
      "Epoch 11/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 57.5718 - acc: 0.6450 - val_loss: 56.9527 - val_acc: 0.7162\n",
      "Epoch 12/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 57.0775 - acc: 0.6221 - val_loss: 56.3947 - val_acc: 0.7297\n",
      "Epoch 13/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 56.4688 - acc: 0.6515 - val_loss: 55.8331 - val_acc: 0.7432\n",
      "Epoch 14/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 55.8271 - acc: 0.6808 - val_loss: 55.2809 - val_acc: 0.7162\n",
      "Epoch 15/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 55.2488 - acc: 0.6971 - val_loss: 54.7276 - val_acc: 0.7162\n",
      "Epoch 16/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 54.7254 - acc: 0.7166 - val_loss: 54.1800 - val_acc: 0.7162\n",
      "Epoch 17/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 54.0994 - acc: 0.7720 - val_loss: 53.6342 - val_acc: 0.7297\n",
      "Epoch 18/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 53.5192 - acc: 0.7850 - val_loss: 53.0870 - val_acc: 0.7162\n",
      "Epoch 19/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 53.0312 - acc: 0.7492 - val_loss: 52.5452 - val_acc: 0.7297\n",
      "Epoch 20/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 52.4237 - acc: 0.8176 - val_loss: 52.0075 - val_acc: 0.7568\n",
      "Epoch 21/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 51.9815 - acc: 0.7622 - val_loss: 51.4730 - val_acc: 0.7568\n",
      "Epoch 22/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 51.3132 - acc: 0.8176 - val_loss: 50.9393 - val_acc: 0.7568\n",
      "Epoch 23/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 50.8156 - acc: 0.8371 - val_loss: 50.4125 - val_acc: 0.7568\n",
      "Epoch 24/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 50.2234 - acc: 0.8664 - val_loss: 49.8953 - val_acc: 0.7568\n",
      "Epoch 25/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 49.7563 - acc: 0.8795 - val_loss: 49.3814 - val_acc: 0.7703\n",
      "Epoch 26/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 49.1876 - acc: 0.8502 - val_loss: 48.8734 - val_acc: 0.7703\n",
      "Epoch 27/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 48.6772 - acc: 0.8827 - val_loss: 48.3695 - val_acc: 0.7703\n",
      "Epoch 28/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 48.1823 - acc: 0.8697 - val_loss: 47.8697 - val_acc: 0.7838\n",
      "Epoch 29/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 47.6433 - acc: 0.9023 - val_loss: 47.3753 - val_acc: 0.7838\n",
      "Epoch 30/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 47.1146 - acc: 0.9251 - val_loss: 46.8914 - val_acc: 0.7703\n",
      "Epoch 31/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 46.5978 - acc: 0.9251 - val_loss: 46.3961 - val_acc: 0.7703\n",
      "Epoch 32/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 46.1147 - acc: 0.9283 - val_loss: 45.9136 - val_acc: 0.7973\n",
      "Epoch 33/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 45.6317 - acc: 0.9251 - val_loss: 45.4415 - val_acc: 0.8243\n",
      "Epoch 34/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 45.1247 - acc: 0.9414 - val_loss: 44.9721 - val_acc: 0.8243\n",
      "Epoch 35/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 44.6448 - acc: 0.9349 - val_loss: 44.5046 - val_acc: 0.8243\n",
      "Epoch 36/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 44.1545 - acc: 0.9544 - val_loss: 44.0411 - val_acc: 0.8243\n",
      "Epoch 37/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 43.7131 - acc: 0.9544 - val_loss: 43.5816 - val_acc: 0.8243\n",
      "Epoch 38/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 43.2140 - acc: 0.9511 - val_loss: 43.1302 - val_acc: 0.8243\n",
      "Epoch 39/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 42.7773 - acc: 0.9479 - val_loss: 42.6660 - val_acc: 0.8243\n",
      "Epoch 40/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 42.2855 - acc: 0.9707 - val_loss: 42.2214 - val_acc: 0.8243\n",
      "Epoch 41/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 41.8569 - acc: 0.9544 - val_loss: 41.7777 - val_acc: 0.8243\n",
      "Epoch 42/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 41.4044 - acc: 0.9577 - val_loss: 41.3347 - val_acc: 0.8243\n",
      "Epoch 43/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 40.9726 - acc: 0.9544 - val_loss: 40.8962 - val_acc: 0.8378\n",
      "Epoch 44/64\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 40.5337 - acc: 0.9642 - val_loss: 40.4596 - val_acc: 0.8378\n",
      "Epoch 45/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 40.0963 - acc: 0.9707 - val_loss: 40.0419 - val_acc: 0.8243\n",
      "Epoch 46/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 39.6106 - acc: 0.9837 - val_loss: 39.5940 - val_acc: 0.8514\n",
      "Epoch 47/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 39.2354 - acc: 0.9674 - val_loss: 39.1779 - val_acc: 0.8378\n",
      "Epoch 48/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 38.7713 - acc: 0.9870 - val_loss: 38.7622 - val_acc: 0.8378\n",
      "Epoch 49/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 38.3538 - acc: 0.9837 - val_loss: 38.3492 - val_acc: 0.8378\n",
      "Epoch 50/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 37.9496 - acc: 0.9707 - val_loss: 37.9405 - val_acc: 0.8378\n",
      "Epoch 51/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 37.5277 - acc: 0.9805 - val_loss: 37.5359 - val_acc: 0.8243\n",
      "Epoch 52/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 37.1241 - acc: 0.9870 - val_loss: 37.1346 - val_acc: 0.8243\n",
      "Epoch 53/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 36.7367 - acc: 0.9805 - val_loss: 36.7383 - val_acc: 0.8243\n",
      "Epoch 54/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 36.3236 - acc: 0.9935 - val_loss: 36.3467 - val_acc: 0.8243\n",
      "Epoch 55/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 35.9439 - acc: 0.9837 - val_loss: 35.9606 - val_acc: 0.8243\n",
      "Epoch 56/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 35.5671 - acc: 0.9837 - val_loss: 35.5808 - val_acc: 0.8243\n",
      "Epoch 57/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 35.1777 - acc: 0.9902 - val_loss: 35.2062 - val_acc: 0.8243\n",
      "Epoch 58/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 34.7984 - acc: 0.9870 - val_loss: 34.8368 - val_acc: 0.8243\n",
      "Epoch 59/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 34.4238 - acc: 0.9935 - val_loss: 34.4711 - val_acc: 0.8243\n",
      "Epoch 60/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 34.0671 - acc: 0.9935 - val_loss: 34.1113 - val_acc: 0.8243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 33.7005 - acc: 0.9935 - val_loss: 33.7556 - val_acc: 0.8378\n",
      "Epoch 62/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 33.3478 - acc: 0.9902 - val_loss: 33.4040 - val_acc: 0.8378\n",
      "Epoch 63/64\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 32.9685 - acc: 1.0000 - val_loss: 33.0573 - val_acc: 0.8378\n",
      "Epoch 64/64\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 32.6412 - acc: 0.9935 - val_loss: 32.7150 - val_acc: 0.8378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8da7f2cb38>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top.fit(train_data, train_labels_onehot,\n",
    "              epochs=64,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.save_weights(top_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model():\n",
    "    opt = keras.optimizers.Adam(lr=lr)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                 \n",
    "    base = applications.DenseNet121(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))\n",
    "    \n",
    "    top = Sequential()\n",
    "\n",
    "    top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    top.add(Dense(nn1, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())\n",
    "    top.add(Dense(nn2, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())\n",
    "    top.add(Dense(nn3, activation='relu', kernel_regularizer=reg))\n",
    "    top.add(BatchNormalization())            \n",
    "    top.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "    top.load_weights(top_weights_file)\n",
    "    top.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    \n",
    "    \n",
    "    model = Model(input= base.input, output= top(base.output))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 230, 230, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 11)           105392971   relu[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 112,430,475\n",
      "Trainable params: 0\n",
      "Non-trainable params: 112,430,475\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = complete_model()\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"Model Summary\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 images belonging to 11 classes.\n",
      "1/1 [==============================] - 16s 16s/step\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=len(test_labels),\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "test_predictions = model.predict_generator(generator, 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.asarray(list(map(str,np.argmax(test_predictions,axis=1)))).reshape(-1,1)\n",
    "test_labels = np.asarray(test_labels).reshape(-1,1)\n",
    "test_acc, test_acc_op = tf.metrics.accuracy(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = sess.run(test_acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy : 0.8888889\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest accuracy : ' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
